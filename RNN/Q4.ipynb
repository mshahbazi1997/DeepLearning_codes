{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ferdosi.txt', encoding='utf-8',header = None)\n",
    "\n",
    "data[0] = data[0].str[:-1]\n",
    "data[1] = data[1].str[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text01 = []\n",
    "for c in data[0]:\n",
    "    \n",
    "    text01 = text01 + list(c)\n",
    "    \n",
    "text02 = []\n",
    "for c in data[0]:\n",
    "    \n",
    "    text02 = text02 + list(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text01 + text02\n",
    "np.save('text', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(np.load('text.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_asci = np.array([ord(c) for c in text]) # Convert to ASCII\n",
    "frq_asci = []\n",
    "\n",
    "for c in range(txt_asci.min(), txt_asci.max()+1):\n",
    "    if (len(np.where(txt_asci == c)[0]) > 0): # Remove infrequent words\n",
    "        frq_asci.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrs = list(chr(x) for x in frq_asci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrs = chrs + ['ุค'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfchar01 = np.array(list(np.shape(item)[0] for item in list(list(item) for item in data[0])))\n",
    "numOfchar02 = np.array(list(np.shape(item)[0] for item in list(list(item) for item in data[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_char = np.max([np.max(numOfchar02),np.max(numOfchar01)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict = dict()\n",
    "for idx,item in enumerate(chrs):\n",
    "    Dict[item] = idx+3\n",
    "Dict['_PAD_'] = 0\n",
    "Dict['_BOM_'] = 1\n",
    "Dict['_EOM_'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inv dictionary\n",
    "InvDict = dict()\n",
    "for item in Dict:\n",
    "    InvDict[Dict[item]] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mes01 = np.array([[0]*(num_char-len(w))+[1]+[Dict[c] for c in w]+[2] for w in data[0]])\n",
    "mes02 = np.array([[1]+[Dict[c] for c in w]+[2]+[0]*(num_char-len(w)) for w in data[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_pre = 0.9\n",
    "\n",
    "all_beyts    = mes01.shape[0]\n",
    "permuted_idx = np.random.permutation(np.arange(all_beyts))\n",
    "num_train    = int(all_beyts*Train_pre)\n",
    "Train_idx    = permuted_idx[:num_train]\n",
    "Test_idx     = permuted_idx[num_train:]\n",
    "\n",
    "Trian_mes01 = mes01[Train_idx,:]\n",
    "Trian_mes02 = mes02[Train_idx,:]\n",
    "Test_mes01 = mes01[Test_idx,:]\n",
    "Test_mes02 = mes02[Test_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "\n",
    "n_batch = 300\n",
    "hidden_num = 128\n",
    "lr_base = 2e-3\n",
    "lr_decay = 0.999\n",
    "lr_floor = 1e-5\n",
    "n_epoch = 100\n",
    "input_embedding_size = 25\n",
    "data_in_batch = 200\n",
    "n_batch = np.int(np.floor(Trian_mes01.shape[0]/data_in_batch))\n",
    "total_num_char = num_char + 2 # for begin and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "####################################################################\n",
    "############################# Train ################################\n",
    "####################################################################\n",
    "\n",
    "# inputs to the model\n",
    "\n",
    "Encoder_Input = tf.placeholder(dtype=tf.int32, shape=(data_in_batch, None))\n",
    "Decoder_Input = tf.placeholder(dtype=tf.int32, shape=(data_in_batch, None))\n",
    "Decoder_Target = tf.placeholder(dtype=tf.int32, shape=(data_in_batch, None))\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([len(chrs) + 3, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "EI = tf.nn.embedding_lookup(embeddings, Encoder_Input)\n",
    "DI = tf.nn.embedding_lookup(embeddings, Decoder_Input)\n",
    "DT = tf.one_hot(Decoder_Target, depth = int(len(chrs) + 3), on_value=1.0, off_value=0.0, axis=-1)\n",
    "\n",
    "#########################################################################\n",
    "############################### Encoder #################################\n",
    "#########################################################################\n",
    "\n",
    "# encoder lstm\n",
    "cell_encoder = tf.contrib.rnn.LSTMCell(num_units=hidden_num, \n",
    "                                       use_peepholes=True, \n",
    "                                       initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                       activation='tanh',\n",
    "                                       name='cell_encoder')\n",
    "\n",
    "all_states_ec, final_state_ec = tf.nn.dynamic_rnn(cell=cell_encoder, \n",
    "                                                  inputs=EI, \n",
    "                                                  initial_state=None,\n",
    "                                                  time_major=False,dtype=tf.float32)\n",
    "\n",
    "#########################################################################\n",
    "############################### Decoder #################################\n",
    "#########################################################################\n",
    "\n",
    "# decoder lstm\n",
    "cell_decoder = tf.contrib.rnn.LSTMCell(num_units=hidden_num, \n",
    "                                       use_peepholes=True, \n",
    "                                       initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                       activation='tanh',\n",
    "                                       name='cell_decoder')\n",
    "\n",
    "all_states_dec, final_state_dec = tf.nn.dynamic_rnn(cell=cell_decoder, \n",
    "                                                     inputs=DI, \n",
    "                                                     initial_state=final_state_ec, \n",
    "                                                     time_major=False,\n",
    "                                                     dtype=tf.float32)\n",
    "# Outputs\n",
    "\n",
    "W = tf.get_variable('W', shape=(hidden_num,int(len(chrs) + 3)),initializer=tf.contrib.layers.xavier_initializer(), dtype=tf.float32)\n",
    "b = tf.get_variable('b', shape=(int(len(chrs) + 3),),initializer=tf.contrib.layers.xavier_initializer(), dtype=tf.float32)\n",
    "\n",
    "Decoder_logit = tf.einsum('ijk,kl', all_states_dec, W)+b\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=DT, logits=Decoder_logit, dim=-1))\n",
    "\n",
    "tf_lr = tf.placeholder(dtype=tf.float32)\n",
    "train_opt = tf.train.RMSPropOptimizer(learning_rate = tf_lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "############################# Test #################################\n",
    "####################################################################\n",
    "\n",
    "\n",
    "# placeholders for feeding in test time:\n",
    "Encoder_Input_test = tf.placeholder(dtype=tf.int32, shape=(data_in_batch, None))\n",
    "Decoder_Input_test = tf.placeholder(dtype=tf.int32, shape=(data_in_batch, None))\n",
    "\n",
    "\n",
    "# Initial_dec_h : activations of hidden layer of LSTM cell\n",
    "# Initial_dec_c : is final output, which can potentially be transfromed with some wrapper\n",
    "Initial_dec_h = tf.placeholder(dtype=tf.float32, shape=(data_in_batch, hidden_num))\n",
    "Initial_dec_c = tf.placeholder(dtype=tf.float32, shape=(data_in_batch, hidden_num))\n",
    "\n",
    "\n",
    "initial_state = tf.nn.rnn_cell.LSTMStateTuple(Initial_dec_c, Initial_dec_h)\n",
    "\n",
    "EI_test = tf.nn.embedding_lookup(embeddings, Encoder_Input_test)\n",
    "DI_test = tf.nn.embedding_lookup(embeddings, Decoder_Input_test)\n",
    "\n",
    "all_states_ec_test, final_state_ec_test = tf.nn.dynamic_rnn(cell=cell_encoder, \n",
    "                                             inputs=EI_test, \n",
    "                                             initial_state=None, \n",
    "                                             time_major=False,\n",
    "                                             dtype=tf.float32)\n",
    "\n",
    "all_states_dec_test, final_state_dec_test = tf.nn.dynamic_rnn(cell=cell_decoder, \n",
    "                                                             inputs=DI_test, \n",
    "                                                             initial_state=initial_state, \n",
    "                                                             time_major=False,\n",
    "                                                             dtype=tf.float32)\n",
    "\n",
    "logits_test = tf.einsum('ijk,kl', all_states_dec_test, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 ====== Loss Train: 3.7704 ====== Loss Test: 3.7576\n",
      "Epoch   0 ====== Loss Train: 2.4570 ====== Loss Test: 2.5772\n",
      "Epoch   0 ====== Loss Train: 1.8086 ====== Loss Test: 2.6940\n",
      "Epoch   1 ====== Loss Train: 1.7456 ====== Loss Test: 2.7656\n",
      "Epoch   1 ====== Loss Train: 1.6173 ====== Loss Test: 3.0580\n",
      "Epoch   1 ====== Loss Train: 1.5948 ====== Loss Test: 3.2536\n",
      "Epoch   2 ====== Loss Train: 1.4898 ====== Loss Test: 3.2906\n",
      "Epoch   2 ====== Loss Train: 1.4584 ====== Loss Test: 3.3763\n",
      "Epoch   2 ====== Loss Train: 1.4558 ====== Loss Test: 3.5052\n",
      "Epoch   3 ====== Loss Train: 1.3642 ====== Loss Test: 3.4750\n",
      "Epoch   3 ====== Loss Train: 1.3758 ====== Loss Test: 3.6535\n",
      "Epoch   3 ====== Loss Train: 1.3298 ====== Loss Test: 3.7764\n",
      "Epoch   4 ====== Loss Train: 1.2740 ====== Loss Test: 3.8273\n",
      "Epoch   4 ====== Loss Train: 1.3191 ====== Loss Test: 3.8532\n",
      "Epoch   4 ====== Loss Train: 1.2532 ====== Loss Test: 4.0419\n",
      "Epoch   5 ====== Loss Train: 1.2320 ====== Loss Test: 3.8808\n",
      "Epoch   5 ====== Loss Train: 1.2190 ====== Loss Test: 3.9540\n",
      "Epoch   5 ====== Loss Train: 1.2044 ====== Loss Test: 4.0423\n",
      "Epoch   6 ====== Loss Train: 1.1621 ====== Loss Test: 4.1410\n",
      "Epoch   6 ====== Loss Train: 1.1792 ====== Loss Test: 4.1386\n",
      "Epoch   6 ====== Loss Train: 1.1583 ====== Loss Test: 4.1593\n",
      "Epoch   7 ====== Loss Train: 1.1261 ====== Loss Test: 4.2324\n",
      "Epoch   7 ====== Loss Train: 1.1506 ====== Loss Test: 4.2763\n",
      "Epoch   7 ====== Loss Train: 1.1272 ====== Loss Test: 4.4172\n",
      "Epoch   8 ====== Loss Train: 1.1223 ====== Loss Test: 4.3100\n",
      "Epoch   8 ====== Loss Train: 1.1205 ====== Loss Test: 4.3016\n",
      "Epoch   8 ====== Loss Train: 1.1009 ====== Loss Test: 4.4873\n",
      "Epoch   9 ====== Loss Train: 1.0716 ====== Loss Test: 4.4769\n",
      "Epoch   9 ====== Loss Train: 1.1305 ====== Loss Test: 4.5370\n",
      "Epoch   9 ====== Loss Train: 1.0768 ====== Loss Test: 4.6103\n",
      "Epoch  10 ====== Loss Train: 1.0582 ====== Loss Test: 4.4185\n",
      "Epoch  10 ====== Loss Train: 1.1031 ====== Loss Test: 4.5413\n",
      "Epoch  10 ====== Loss Train: 1.0699 ====== Loss Test: 4.5718\n",
      "Epoch  11 ====== Loss Train: 1.0327 ====== Loss Test: 4.6621\n",
      "Epoch  11 ====== Loss Train: 1.0713 ====== Loss Test: 4.6404\n",
      "Epoch  11 ====== Loss Train: 1.0740 ====== Loss Test: 4.8628\n",
      "Epoch  12 ====== Loss Train: 1.0255 ====== Loss Test: 4.6079\n",
      "Epoch  12 ====== Loss Train: 1.0575 ====== Loss Test: 4.6688\n",
      "Epoch  12 ====== Loss Train: 1.0455 ====== Loss Test: 4.7513\n",
      "Epoch  13 ====== Loss Train: 1.0134 ====== Loss Test: 4.6789\n",
      "Epoch  13 ====== Loss Train: 1.0537 ====== Loss Test: 4.9008\n",
      "Epoch  13 ====== Loss Train: 1.0283 ====== Loss Test: 4.8889\n",
      "Epoch  14 ====== Loss Train: 0.9970 ====== Loss Test: 4.7925\n",
      "Epoch  14 ====== Loss Train: 1.0420 ====== Loss Test: 4.7183\n",
      "Epoch  14 ====== Loss Train: 1.0191 ====== Loss Test: 4.8558\n",
      "Epoch  15 ====== Loss Train: 0.9986 ====== Loss Test: 4.8507\n",
      "Epoch  15 ====== Loss Train: 1.0391 ====== Loss Test: 4.8724\n",
      "Epoch  15 ====== Loss Train: 1.0061 ====== Loss Test: 4.9552\n",
      "Epoch  16 ====== Loss Train: 0.9922 ====== Loss Test: 5.0329\n",
      "Epoch  16 ====== Loss Train: 1.0221 ====== Loss Test: 4.9757\n",
      "Epoch  16 ====== Loss Train: 1.0103 ====== Loss Test: 4.9212\n",
      "Epoch  17 ====== Loss Train: 1.0398 ====== Loss Test: 4.8736\n",
      "Epoch  17 ====== Loss Train: 1.0165 ====== Loss Test: 5.0701\n",
      "Epoch  17 ====== Loss Train: 0.9938 ====== Loss Test: 4.9936\n",
      "Epoch  18 ====== Loss Train: 0.9756 ====== Loss Test: 4.8595\n",
      "Epoch  18 ====== Loss Train: 1.0123 ====== Loss Test: 4.9835\n",
      "Epoch  18 ====== Loss Train: 0.9974 ====== Loss Test: 5.0760\n",
      "Epoch  19 ====== Loss Train: 0.9695 ====== Loss Test: 5.1161\n",
      "Epoch  19 ====== Loss Train: 1.0524 ====== Loss Test: 5.0500\n",
      "Epoch  19 ====== Loss Train: 0.9916 ====== Loss Test: 5.1205\n",
      "Epoch  20 ====== Loss Train: 0.9672 ====== Loss Test: 5.0885\n",
      "Epoch  20 ====== Loss Train: 0.9959 ====== Loss Test: 5.0533\n",
      "Epoch  20 ====== Loss Train: 0.9797 ====== Loss Test: 5.2469\n",
      "Epoch  21 ====== Loss Train: 0.9532 ====== Loss Test: 5.1265\n",
      "Epoch  21 ====== Loss Train: 0.9858 ====== Loss Test: 4.9704\n",
      "Epoch  21 ====== Loss Train: 0.9713 ====== Loss Test: 5.1046\n",
      "Epoch  22 ====== Loss Train: 0.9701 ====== Loss Test: 5.3148\n",
      "Epoch  22 ====== Loss Train: 0.9749 ====== Loss Test: 5.2694\n",
      "Epoch  22 ====== Loss Train: 0.9694 ====== Loss Test: 5.0991\n",
      "Epoch  23 ====== Loss Train: 0.9574 ====== Loss Test: 5.2414\n",
      "Epoch  23 ====== Loss Train: 0.9718 ====== Loss Test: 5.1717\n",
      "Epoch  23 ====== Loss Train: 0.9916 ====== Loss Test: 5.2106\n",
      "Epoch  24 ====== Loss Train: 0.9372 ====== Loss Test: 5.2346\n",
      "Epoch  24 ====== Loss Train: 0.9738 ====== Loss Test: 5.2418\n",
      "Epoch  24 ====== Loss Train: 0.9889 ====== Loss Test: 5.1961\n",
      "Epoch  25 ====== Loss Train: 0.9338 ====== Loss Test: 5.2519\n",
      "Epoch  25 ====== Loss Train: 0.9733 ====== Loss Test: 5.3208\n",
      "Epoch  25 ====== Loss Train: 1.0215 ====== Loss Test: 5.3099\n",
      "Epoch  26 ====== Loss Train: 0.9320 ====== Loss Test: 5.2609\n",
      "Epoch  26 ====== Loss Train: 0.9607 ====== Loss Test: 5.2976\n",
      "Epoch  26 ====== Loss Train: 0.9532 ====== Loss Test: 5.2716\n",
      "Epoch  27 ====== Loss Train: 0.9325 ====== Loss Test: 5.3938\n",
      "Epoch  27 ====== Loss Train: 0.9569 ====== Loss Test: 5.3789\n",
      "Epoch  27 ====== Loss Train: 0.9472 ====== Loss Test: 5.4421\n",
      "Epoch  28 ====== Loss Train: 0.9252 ====== Loss Test: 5.2687\n",
      "Epoch  28 ====== Loss Train: 0.9887 ====== Loss Test: 5.2392\n",
      "Epoch  28 ====== Loss Train: 0.9481 ====== Loss Test: 5.3369\n",
      "Epoch  29 ====== Loss Train: 0.9309 ====== Loss Test: 5.4052\n",
      "Epoch  29 ====== Loss Train: 0.9513 ====== Loss Test: 5.4011\n",
      "Epoch  29 ====== Loss Train: 0.9385 ====== Loss Test: 5.3072\n",
      "Epoch  30 ====== Loss Train: 0.9209 ====== Loss Test: 5.3702\n",
      "Epoch  30 ====== Loss Train: 0.9468 ====== Loss Test: 5.3442\n",
      "Epoch  30 ====== Loss Train: 0.9323 ====== Loss Test: 5.4621\n",
      "Epoch  31 ====== Loss Train: 0.9149 ====== Loss Test: 5.3731\n",
      "Epoch  31 ====== Loss Train: 0.9917 ====== Loss Test: 5.6747\n",
      "Epoch  31 ====== Loss Train: 0.9301 ====== Loss Test: 5.5821\n",
      "Epoch  32 ====== Loss Train: 0.9551 ====== Loss Test: 5.2527\n",
      "Epoch  32 ====== Loss Train: 0.9396 ====== Loss Test: 5.5950\n",
      "Epoch  32 ====== Loss Train: 0.9265 ====== Loss Test: 5.4998\n",
      "Epoch  33 ====== Loss Train: 0.9106 ====== Loss Test: 5.4250\n",
      "Epoch  33 ====== Loss Train: 0.9375 ====== Loss Test: 5.4675\n",
      "Epoch  33 ====== Loss Train: 0.9243 ====== Loss Test: 5.6039\n",
      "Epoch  34 ====== Loss Train: 0.9092 ====== Loss Test: 5.5250\n",
      "Epoch  34 ====== Loss Train: 0.9346 ====== Loss Test: 5.5309\n",
      "Epoch  34 ====== Loss Train: 0.9219 ====== Loss Test: 5.7037\n",
      "Epoch  35 ====== Loss Train: 0.9073 ====== Loss Test: 5.5574\n",
      "Epoch  35 ====== Loss Train: 0.9330 ====== Loss Test: 5.6191\n",
      "Epoch  35 ====== Loss Train: 0.9204 ====== Loss Test: 5.6277\n",
      "Epoch  36 ====== Loss Train: 0.9060 ====== Loss Test: 5.5533\n",
      "Epoch  36 ====== Loss Train: 0.9306 ====== Loss Test: 5.4660\n",
      "Epoch  36 ====== Loss Train: 0.9178 ====== Loss Test: 5.5401\n",
      "Epoch  37 ====== Loss Train: 0.9041 ====== Loss Test: 5.5237\n",
      "Epoch  37 ====== Loss Train: 0.9287 ====== Loss Test: 5.5542\n",
      "Epoch  37 ====== Loss Train: 0.9285 ====== Loss Test: 5.6067\n",
      "Epoch  38 ====== Loss Train: 1.0376 ====== Loss Test: 5.4776\n",
      "Epoch  38 ====== Loss Train: 0.9898 ====== Loss Test: 5.6628\n",
      "Epoch  38 ====== Loss Train: 0.9148 ====== Loss Test: 5.6982\n",
      "Epoch  39 ====== Loss Train: 0.9020 ====== Loss Test: 5.4748\n",
      "Epoch  39 ====== Loss Train: 0.9254 ====== Loss Test: 5.6000\n",
      "Epoch  39 ====== Loss Train: 0.9227 ====== Loss Test: 5.5810\n",
      "Epoch  40 ====== Loss Train: 0.9082 ====== Loss Test: 5.4763\n",
      "Epoch  40 ====== Loss Train: 0.9237 ====== Loss Test: 5.6595\n",
      "Epoch  40 ====== Loss Train: 0.9098 ====== Loss Test: 5.6030\n",
      "Epoch  41 ====== Loss Train: 0.8972 ====== Loss Test: 5.6447\n",
      "Epoch  41 ====== Loss Train: 0.9215 ====== Loss Test: 5.5402\n",
      "Epoch  41 ====== Loss Train: 0.9075 ====== Loss Test: 5.7392\n",
      "Epoch  42 ====== Loss Train: 0.8960 ====== Loss Test: 5.5667\n",
      "Epoch  42 ====== Loss Train: 0.9192 ====== Loss Test: 5.6714\n",
      "Epoch  42 ====== Loss Train: 0.9059 ====== Loss Test: 5.7088\n",
      "Epoch  43 ====== Loss Train: 0.8951 ====== Loss Test: 5.6065\n",
      "Epoch  43 ====== Loss Train: 0.9175 ====== Loss Test: 5.6482\n",
      "Epoch  43 ====== Loss Train: 0.9046 ====== Loss Test: 5.5445\n",
      "Epoch  44 ====== Loss Train: 1.0078 ====== Loss Test: 5.7919\n",
      "Epoch  44 ====== Loss Train: 0.9158 ====== Loss Test: 5.6527\n",
      "Epoch  44 ====== Loss Train: 0.9029 ====== Loss Test: 5.6917\n",
      "Epoch  45 ====== Loss Train: 0.8928 ====== Loss Test: 5.6089\n",
      "Epoch  45 ====== Loss Train: 0.9143 ====== Loss Test: 5.6997\n",
      "Epoch  45 ====== Loss Train: 0.9014 ====== Loss Test: 5.7189\n",
      "Epoch  46 ====== Loss Train: 0.8917 ====== Loss Test: 5.7045\n",
      "Epoch  46 ====== Loss Train: 0.9131 ====== Loss Test: 5.7103\n",
      "Epoch  46 ====== Loss Train: 0.9001 ====== Loss Test: 5.6436\n",
      "Epoch  47 ====== Loss Train: 0.8907 ====== Loss Test: 5.6722\n",
      "Epoch  47 ====== Loss Train: 0.9113 ====== Loss Test: 5.6546\n",
      "Epoch  47 ====== Loss Train: 0.8989 ====== Loss Test: 5.8041\n",
      "Epoch  48 ====== Loss Train: 0.8890 ====== Loss Test: 5.6520\n",
      "Epoch  48 ====== Loss Train: 0.9107 ====== Loss Test: 5.7145\n",
      "Epoch  48 ====== Loss Train: 0.8971 ====== Loss Test: 5.7163\n",
      "Epoch  49 ====== Loss Train: 0.8893 ====== Loss Test: 5.7443\n",
      "Epoch  49 ====== Loss Train: 0.9096 ====== Loss Test: 5.6749\n",
      "Epoch  49 ====== Loss Train: 0.8966 ====== Loss Test: 5.8354\n",
      "Epoch  50 ====== Loss Train: 0.8869 ====== Loss Test: 5.7802\n",
      "Epoch  50 ====== Loss Train: 0.9082 ====== Loss Test: 5.8015\n",
      "Epoch  50 ====== Loss Train: 0.8933 ====== Loss Test: 5.7072\n",
      "Epoch  51 ====== Loss Train: 0.8847 ====== Loss Test: 5.7753\n",
      "Epoch  51 ====== Loss Train: 0.9063 ====== Loss Test: 5.7834\n",
      "Epoch  51 ====== Loss Train: 0.8919 ====== Loss Test: 5.7353\n",
      "Epoch  52 ====== Loss Train: 0.8840 ====== Loss Test: 5.7805\n",
      "Epoch  52 ====== Loss Train: 0.9047 ====== Loss Test: 5.7166\n",
      "Epoch  52 ====== Loss Train: 0.8908 ====== Loss Test: 5.7557\n",
      "Epoch  53 ====== Loss Train: 0.8828 ====== Loss Test: 5.6989\n",
      "Epoch  53 ====== Loss Train: 0.9036 ====== Loss Test: 5.8621\n",
      "Epoch  53 ====== Loss Train: 0.8901 ====== Loss Test: 5.8511\n",
      "Epoch  54 ====== Loss Train: 0.8817 ====== Loss Test: 5.8133\n",
      "Epoch  54 ====== Loss Train: 0.9030 ====== Loss Test: 5.8895\n",
      "Epoch  54 ====== Loss Train: 0.8887 ====== Loss Test: 5.8018\n",
      "Epoch  55 ====== Loss Train: 0.8810 ====== Loss Test: 5.7551\n",
      "Epoch  55 ====== Loss Train: 0.9020 ====== Loss Test: 5.7673\n",
      "Epoch  55 ====== Loss Train: 0.8879 ====== Loss Test: 5.8347\n",
      "Epoch  56 ====== Loss Train: 0.8806 ====== Loss Test: 5.8221\n",
      "Epoch  56 ====== Loss Train: 0.9009 ====== Loss Test: 5.8798\n",
      "Epoch  56 ====== Loss Train: 0.8868 ====== Loss Test: 5.9109\n",
      "Epoch  57 ====== Loss Train: 0.8793 ====== Loss Test: 5.7885\n",
      "Epoch  57 ====== Loss Train: 0.9002 ====== Loss Test: 5.7299\n",
      "Epoch  57 ====== Loss Train: 0.8858 ====== Loss Test: 5.8915\n",
      "Epoch  58 ====== Loss Train: 0.8789 ====== Loss Test: 5.8405\n",
      "Epoch  58 ====== Loss Train: 0.9034 ====== Loss Test: 5.7849\n",
      "Epoch  58 ====== Loss Train: 0.8845 ====== Loss Test: 5.8420\n",
      "Epoch  59 ====== Loss Train: 0.8776 ====== Loss Test: 5.8626\n",
      "Epoch  59 ====== Loss Train: 0.8990 ====== Loss Test: 5.8742\n",
      "Epoch  59 ====== Loss Train: 0.8838 ====== Loss Test: 5.9337\n",
      "Epoch  60 ====== Loss Train: 0.8773 ====== Loss Test: 5.8775\n",
      "Epoch  60 ====== Loss Train: 0.8972 ====== Loss Test: 5.9048\n",
      "Epoch  60 ====== Loss Train: 0.8818 ====== Loss Test: 5.9471\n",
      "Epoch  61 ====== Loss Train: 0.8754 ====== Loss Test: 5.8852\n",
      "Epoch  61 ====== Loss Train: 0.8958 ====== Loss Test: 5.8188\n",
      "Epoch  61 ====== Loss Train: 0.8806 ====== Loss Test: 6.0889\n",
      "Epoch  62 ====== Loss Train: 0.8751 ====== Loss Test: 5.9055\n",
      "Epoch  62 ====== Loss Train: 0.8952 ====== Loss Test: 5.8390\n",
      "Epoch  62 ====== Loss Train: 0.8799 ====== Loss Test: 5.9377\n",
      "Epoch  63 ====== Loss Train: 0.8740 ====== Loss Test: 5.8519\n",
      "Epoch  63 ====== Loss Train: 0.8950 ====== Loss Test: 5.8349\n",
      "Epoch  63 ====== Loss Train: 0.8795 ====== Loss Test: 5.9520\n",
      "Epoch  64 ====== Loss Train: 0.8731 ====== Loss Test: 5.8485\n",
      "Epoch  64 ====== Loss Train: 0.8938 ====== Loss Test: 5.9584\n",
      "Epoch  64 ====== Loss Train: 0.8785 ====== Loss Test: 5.8799\n",
      "Epoch  65 ====== Loss Train: 0.8730 ====== Loss Test: 6.0154\n",
      "Epoch  65 ====== Loss Train: 0.8935 ====== Loss Test: 5.8739\n",
      "Epoch  65 ====== Loss Train: 0.8779 ====== Loss Test: 5.9946\n",
      "Epoch  66 ====== Loss Train: 0.8720 ====== Loss Test: 5.9710\n",
      "Epoch  66 ====== Loss Train: 0.8927 ====== Loss Test: 5.9761\n",
      "Epoch  66 ====== Loss Train: 0.8769 ====== Loss Test: 6.0219\n",
      "Epoch  67 ====== Loss Train: 0.8717 ====== Loss Test: 6.0668\n",
      "Epoch  67 ====== Loss Train: 0.8922 ====== Loss Test: 6.0109\n",
      "Epoch  67 ====== Loss Train: 0.8766 ====== Loss Test: 6.0680\n",
      "Epoch  68 ====== Loss Train: 0.8710 ====== Loss Test: 5.9499\n",
      "Epoch  68 ====== Loss Train: 0.8912 ====== Loss Test: 5.9663\n",
      "Epoch  68 ====== Loss Train: 0.8761 ====== Loss Test: 6.0855\n",
      "Epoch  69 ====== Loss Train: 0.8705 ====== Loss Test: 6.0206\n",
      "Epoch  69 ====== Loss Train: 0.8906 ====== Loss Test: 5.9320\n",
      "Epoch  69 ====== Loss Train: 0.8754 ====== Loss Test: 6.0113\n",
      "Epoch  70 ====== Loss Train: 0.8699 ====== Loss Test: 5.8954\n",
      "Epoch  70 ====== Loss Train: 0.8897 ====== Loss Test: 5.9881\n",
      "Epoch  70 ====== Loss Train: 0.8741 ====== Loss Test: 5.9620\n",
      "Epoch  71 ====== Loss Train: 0.9213 ====== Loss Test: 6.0127\n",
      "Epoch  71 ====== Loss Train: 0.8886 ====== Loss Test: 6.0592\n",
      "Epoch  71 ====== Loss Train: 0.8731 ====== Loss Test: 6.0271\n",
      "Epoch  72 ====== Loss Train: 0.8682 ====== Loss Test: 6.0798\n",
      "Epoch  72 ====== Loss Train: 0.8877 ====== Loss Test: 6.0484\n",
      "Epoch  72 ====== Loss Train: 0.8989 ====== Loss Test: 6.0594\n",
      "Epoch  73 ====== Loss Train: 0.8678 ====== Loss Test: 5.9578\n",
      "Epoch  73 ====== Loss Train: 0.8874 ====== Loss Test: 6.0416\n",
      "Epoch  73 ====== Loss Train: 0.8724 ====== Loss Test: 6.1070\n",
      "Epoch  74 ====== Loss Train: 0.8672 ====== Loss Test: 6.0330\n",
      "Epoch  74 ====== Loss Train: 0.8870 ====== Loss Test: 5.9287\n",
      "Epoch  74 ====== Loss Train: 0.8838 ====== Loss Test: 6.1345\n",
      "Epoch  75 ====== Loss Train: 0.8668 ====== Loss Test: 5.9550\n",
      "Epoch  75 ====== Loss Train: 0.8867 ====== Loss Test: 5.9521\n",
      "Epoch  75 ====== Loss Train: 0.8714 ====== Loss Test: 5.9972\n",
      "Epoch  76 ====== Loss Train: 0.8662 ====== Loss Test: 6.0354\n",
      "Epoch  76 ====== Loss Train: 0.8861 ====== Loss Test: 5.9404\n",
      "Epoch  76 ====== Loss Train: 0.8965 ====== Loss Test: 6.0127\n",
      "Epoch  77 ====== Loss Train: 0.8656 ====== Loss Test: 6.1312\n",
      "Epoch  77 ====== Loss Train: 0.8856 ====== Loss Test: 6.0645\n",
      "Epoch  77 ====== Loss Train: 0.8985 ====== Loss Test: 6.1100\n",
      "Epoch  78 ====== Loss Train: 0.8652 ====== Loss Test: 5.9410\n",
      "Epoch  78 ====== Loss Train: 0.8852 ====== Loss Test: 6.0428\n",
      "Epoch  78 ====== Loss Train: 0.8851 ====== Loss Test: 5.9954\n",
      "Epoch  79 ====== Loss Train: 0.8648 ====== Loss Test: 5.9760\n",
      "Epoch  79 ====== Loss Train: 0.8845 ====== Loss Test: 5.9640\n",
      "Epoch  79 ====== Loss Train: 0.8700 ====== Loss Test: 6.0128\n",
      "Epoch  80 ====== Loss Train: 0.8642 ====== Loss Test: 5.9790\n",
      "Epoch  80 ====== Loss Train: 0.8839 ====== Loss Test: 6.0449\n",
      "Epoch  80 ====== Loss Train: 0.8684 ====== Loss Test: 6.1461\n",
      "Epoch  81 ====== Loss Train: 0.8634 ====== Loss Test: 5.9932\n",
      "Epoch  81 ====== Loss Train: 0.8832 ====== Loss Test: 5.9675\n",
      "Epoch  81 ====== Loss Train: 0.8676 ====== Loss Test: 6.0920\n",
      "Epoch  82 ====== Loss Train: 0.8629 ====== Loss Test: 6.1118\n",
      "Epoch  82 ====== Loss Train: 0.8828 ====== Loss Test: 6.1473\n",
      "Epoch  82 ====== Loss Train: 0.8671 ====== Loss Test: 6.0684\n",
      "Epoch  83 ====== Loss Train: 0.8626 ====== Loss Test: 6.1479\n",
      "Epoch  83 ====== Loss Train: 0.8823 ====== Loss Test: 6.0322\n",
      "Epoch  83 ====== Loss Train: 0.8665 ====== Loss Test: 6.1200\n",
      "Epoch  84 ====== Loss Train: 0.8622 ====== Loss Test: 6.0952\n",
      "Epoch  84 ====== Loss Train: 0.8818 ====== Loss Test: 6.1997\n",
      "Epoch  84 ====== Loss Train: 0.8662 ====== Loss Test: 6.1533\n",
      "Epoch  85 ====== Loss Train: 0.8618 ====== Loss Test: 6.1698\n",
      "Epoch  85 ====== Loss Train: 0.8812 ====== Loss Test: 6.1605\n",
      "Epoch  85 ====== Loss Train: 0.8659 ====== Loss Test: 6.0806\n",
      "Epoch  86 ====== Loss Train: 0.8614 ====== Loss Test: 6.2068\n",
      "Epoch  86 ====== Loss Train: 0.8806 ====== Loss Test: 6.1581\n",
      "Epoch  86 ====== Loss Train: 0.8654 ====== Loss Test: 6.1470\n",
      "Epoch  87 ====== Loss Train: 0.8610 ====== Loss Test: 6.0905\n",
      "Epoch  87 ====== Loss Train: 0.8804 ====== Loss Test: 6.3157\n",
      "Epoch  87 ====== Loss Train: 0.8652 ====== Loss Test: 6.1218\n",
      "Epoch  88 ====== Loss Train: 0.8607 ====== Loss Test: 6.1721\n",
      "Epoch  88 ====== Loss Train: 0.8800 ====== Loss Test: 6.2546\n",
      "Epoch  88 ====== Loss Train: 0.8649 ====== Loss Test: 6.2652\n",
      "Epoch  89 ====== Loss Train: 0.8603 ====== Loss Test: 6.1972\n",
      "Epoch  89 ====== Loss Train: 0.8795 ====== Loss Test: 6.2694\n",
      "Epoch  89 ====== Loss Train: 0.8644 ====== Loss Test: 6.2713\n",
      "Epoch  90 ====== Loss Train: 0.8600 ====== Loss Test: 6.1892\n",
      "Epoch  90 ====== Loss Train: 0.8790 ====== Loss Test: 6.2143\n",
      "Epoch  90 ====== Loss Train: 0.8635 ====== Loss Test: 6.1593\n",
      "Epoch  91 ====== Loss Train: 0.8590 ====== Loss Test: 6.1734\n",
      "Epoch  91 ====== Loss Train: 0.8782 ====== Loss Test: 6.2050\n",
      "Epoch  91 ====== Loss Train: 0.8629 ====== Loss Test: 6.3812\n",
      "Epoch  92 ====== Loss Train: 0.8588 ====== Loss Test: 6.0433\n",
      "Epoch  92 ====== Loss Train: 0.8779 ====== Loss Test: 6.1195\n",
      "Epoch  92 ====== Loss Train: 0.8624 ====== Loss Test: 6.1737\n",
      "Epoch  93 ====== Loss Train: 0.8586 ====== Loss Test: 6.2150\n",
      "Epoch  93 ====== Loss Train: 0.8774 ====== Loss Test: 6.3575\n",
      "Epoch  93 ====== Loss Train: 0.8641 ====== Loss Test: 6.3218\n",
      "Epoch  94 ====== Loss Train: 0.8584 ====== Loss Test: 6.2327\n",
      "Epoch  94 ====== Loss Train: 0.8770 ====== Loss Test: 6.2856\n",
      "Epoch  94 ====== Loss Train: 0.8616 ====== Loss Test: 6.2758\n",
      "Epoch  95 ====== Loss Train: 0.8580 ====== Loss Test: 6.1839\n",
      "Epoch  95 ====== Loss Train: 0.8766 ====== Loss Test: 6.3330\n",
      "Epoch  95 ====== Loss Train: 0.8613 ====== Loss Test: 6.1882\n",
      "Epoch  96 ====== Loss Train: 0.8578 ====== Loss Test: 6.2264\n",
      "Epoch  96 ====== Loss Train: 0.8763 ====== Loss Test: 6.3804\n",
      "Epoch  96 ====== Loss Train: 0.8613 ====== Loss Test: 6.2664\n",
      "Epoch  97 ====== Loss Train: 0.8668 ====== Loss Test: 6.3476\n",
      "Epoch  97 ====== Loss Train: 0.8758 ====== Loss Test: 6.1448\n",
      "Epoch  97 ====== Loss Train: 0.8607 ====== Loss Test: 6.2738\n",
      "Epoch  98 ====== Loss Train: 0.8618 ====== Loss Test: 6.2914\n",
      "Epoch  98 ====== Loss Train: 0.8753 ====== Loss Test: 6.2328\n",
      "Epoch  98 ====== Loss Train: 0.8602 ====== Loss Test: 6.3212\n",
      "Epoch  99 ====== Loss Train: 0.8577 ====== Loss Test: 6.2789\n",
      "Epoch  99 ====== Loss Train: 0.8751 ====== Loss Test: 6.3441\n",
      "Epoch  99 ====== Loss Train: 0.8600 ====== Loss Test: 6.4119\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "\n",
    "lr = lr_base\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for j in range(n_batch):\n",
    "        \n",
    "        # adjusting learning rete \n",
    "        if i%10==0:\n",
    "            lr = lr*lr_decay\n",
    "            if lr < lr_floor:\n",
    "                lr = lr_floor\n",
    "        mes1_minibatch = Trian_mes01[(j)*data_in_batch:min((j+1)*data_in_batch,Trian_mes01.shape[0]),:]\n",
    "        mes2_minibatch = Trian_mes02[(j)*data_in_batch:min((j+1)*data_in_batch,Trian_mes01.shape[0]),:]\n",
    "                \n",
    "        target = mes2_minibatch\n",
    "        target = np.roll(target,-1,axis=1)\n",
    "        target[0,-1] = 0\n",
    "        \n",
    "        loss_train,_ = session.run([loss,train_opt], feed_dict={Encoder_Input: mes1_minibatch,\n",
    "                                                               Decoder_Input: mes2_minibatch,\n",
    "                                                               Decoder_Target: target,\n",
    "                                                               tf_lr: lr})\n",
    "                \n",
    "        test_idx = np.random.randint(0, Test_mes01.shape[0], size = data_in_batch)\n",
    "        \n",
    "        target_test = Test_mes02[test_idx,:]\n",
    "        target_test = np.roll(target,-1,axis=1)\n",
    "        target_test[0,-1] = 0\n",
    "        \n",
    "        loss_test = session.run(loss,feed_dict={Encoder_Input: Test_mes01[test_idx,:],\n",
    "                                                    Decoder_Input: Test_mes02[test_idx,:],\n",
    "                                                    Decoder_Target: target_test})\n",
    "        \n",
    "        if j%100==0:\n",
    "            print(\"Epoch %3i\"%i, \"====== Loss Train: %.4f\" %loss_train,\"====== Loss Test: %.4f\"%loss_test)\n",
    "            train_loss = train_loss + [loss_train]\n",
    "            test_loss = test_loss + [loss_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1 = plt.figure()  \n",
    "plt.plot(range(len(train_loss)), train_loss, label='loss on train')\n",
    "plt.plot(range(len(test_loss)), test_loss, label='loss on test')\n",
    "\n",
    "plt.xlabel('iteration x 100')\n",
    "plt.ylabel('loss')\n",
    "plt.title(\"\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_mes01 = Trian_mes01[0:data_in_batch,:]\n",
    "\n",
    "num_of_input = np.shape(Input_mes01)[0]\n",
    "DecInitState = session.run(final_state_ec_test, feed_dict={Encoder_Input_test: Input_mes01})\n",
    "\n",
    "# output mesra'\n",
    "Output = np.array([[Dict['_BOM_']]*num_of_input])\n",
    "\n",
    "logits, new_state = session.run([logits_test, final_state_dec_test], \n",
    "                                    feed_dict={Decoder_Input_test: Output[-1][np.newaxis].T,\n",
    "                                               Encoder_Input: Input_mes01,\n",
    "                                               Decoder_Input: Output[-1][np.newaxis].T,\n",
    "                                               Initial_dec_c: DecInitState[0],\n",
    "                                               Initial_dec_h: DecInitState[1]})\n",
    "\n",
    "# in each state we get a char\n",
    "\n",
    "Output_Char = np.argmax(logits, axis=-1)\n",
    "\n",
    "Output = np.concatenate((Output,Output_Char.T))\n",
    "counter = 0 \n",
    "\n",
    "stop_length = 0\n",
    "\n",
    "while not all(Output[-1] == Dict['_PAD_'])and stop_length < 50:\n",
    "    stop_length += 1\n",
    "    logits, new_state = session.run([logits_test, final_state_dec_test], \n",
    "                                        feed_dict={Decoder_Input_test: Output[-1][np.newaxis].T,\n",
    "                                                   Encoder_Input: Input_mes01,\n",
    "                                                   Decoder_Input: Output[-1][np.newaxis].T,\n",
    "                                                   Initial_dec_h: new_state[1],\n",
    "                                                   Initial_dec_c: new_state[0]})\n",
    "    Output_Char = np.argmax(logits, axis=-1)\n",
    "    Output = np.concatenate((Output,Output_Char.T))\n",
    "\n",
    "Output = Output.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_of_model = [''.join([InvDict[x] for x in Output[idx,:] if not x==0 and not x==1 and not x==2]) for idx in range(50)]\n",
    "np.save('Output_of_model', Output_of_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ุจู ุงุฑุงู ุจู ุฏุฏุงุฑ ุจุง ุงู ุฒูู',\n",
       " 'ุจุฑุงู ุณุงู ฺฉู ุจุฑ ุชุฎุช ุดุงู ุจุฌูุด',\n",
       " 'ุจู ุงุฑุงู ุจุฑุงูุฏ ุฒ ุงุฑุงู ุจุฑู',\n",
       " 'ุจุฑุงู ุชุฎุช ุดุงู ุจุฑุงูุฏ ุฒ ุฏุงุฑ',\n",
       " 'ุจุฑุงู ุชุงุฌ ู ุชุฎุช ู ุจู ุฑู ู ุณูพุงู',\n",
       " 'ุจู ูพุด ุงูุฏุฑูู ุดุงู ุจุง ุงู ุจุฏ',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุฏุฏุงุฑ ฺฉุฑุฏ',\n",
       " 'ุจู ูพุด ุณูพู ุฑุง ุจู ุฏู ุจุฑ ฺฏุฐุดุช',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุฏูุงุฑ ุฎุดฺฉ',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุฏุฏุงุฑ ุดุงุฏ',\n",
       " 'ุจู ูพุด ุงูุฏุฑูู ุฑุง ุจุงุฏ ุดุฏุงุฑ',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุงุฑุงู ููุงูุฏ',\n",
       " 'ุจู ูพุด ุณูพู ุฑุง ุจู ุฏุฏุงุฑ ฺฉุฑุฏ',\n",
       " 'ุจู ูพุด ุงูุฏุฑูู ุฑุง ุงู ุงูุชุงู',\n",
       " 'ุจุฑุงู ูุงููุฑ ุดูุฑุงุฑ ุฌูุงู',\n",
       " 'ุจุฑุงู ูุงููุฑ ุดูุฑุงุฑ ุณูพ',\n",
       " 'ุจู ุฏุฏุงุฑ ุงู ุจุฑ ุณุฑ ุงุฒ ุฏุฑุฏ ู ุฌูุช',\n",
       " 'ฺฉู ุงุฒ ุฑุงู ุจุฏุงุฑ ุจุฑ ูุง ุณุฑุงู',\n",
       " 'ุจู ูพุด ุณูพู ุฑุง ุจู ุฏู ุจุฑ ูู ุงู',\n",
       " 'ุจู ุงุฑุงู ุจู ุงุฑุงู ูุจุงุฏ ูู',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุฑ ุณุฑ ุจุฑุงูุฏ ุฒ ุฎุดุช',\n",
       " 'ุจู ูพุด ุงูุฏุฑูู ุฑุง ุจุงุฏ ุดุฏุงุฑ',\n",
       " 'ูุจูุฏ ุจู ุฏูุงุฑ ู ุจุง ุงู ุจู ุฏุณุช',\n",
       " 'ุจู ูพุด ุณูพู ุฑุง ุจู ุฏู ุจุฑ ฺฏุฑุฒ',\n",
       " 'ุจู ุฏุฏุงุฑ ุงู ุจุฑ ุณุฑ ุงุฒ ุฏุงุฏ ุฑุงู',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุงุฑุงู ูฺฏุฑ',\n",
       " 'ุณุฑ ุชุฎุช ุดุงู ุจู ุงุฑุงู ุจู ุฏุดุช',\n",
       " 'ุจุฑุงู ูุงููุฑ ุดูุฑุงุฑ ุฌูุง',\n",
       " 'ุจู ุงุฑุงู ุจู ุฏุฏุงุฑ ุจุฑ ุณุฑ ุจูุฏ',\n",
       " 'ฺฉู ุง ูุงูุฏุงุฑุงู ู ุจุฑ ูพุด ฺฏุฑู',\n",
       " 'ุจุฑุงู ุจุงุฑู ู ุฑุง ู ุจุง ุงู ุฑุง',\n",
       " 'ุจู ูพุด ุณูพู ุฑุง ุจู ุงุฑุงู ุฒูู',\n",
       " 'ุจู ฺฏุฑุฏ ุงูุฏุฑูู ุดุงู ุจุง ุงู ุจุฏุฏ',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุฏุฏุงุฑ ูุณุช',\n",
       " 'ุจุฑุงู ูุงููุฑ ุดูุฑุงุฑ ุฌูุง',\n",
       " 'ฺฉู ุง ูุงูุฏุงุฑุงู ู ุงู ูุงูุฏ',\n",
       " 'ุจู ุงุฑุงู ุจู ุฏุฏุงุฑ ุจุง ุงู ุจุฑู',\n",
       " 'ุจุฑุงู ุจุงุฑู ุจุฑ ูุฑุฏ ุจุฑ ูพุด ุจุงุฏ',\n",
       " 'ุจู ุฏูุงุฑ ู ุจุง ุงู ุจู ุฏุฏุงุฑ ฺฉุฑุฏ',\n",
       " 'ุจุฑุงู ูุงููุฑ ุดูุฑุงุฑ ุฌูุงุฏ',\n",
       " 'ฺฉู ุงุฒ ุฑุงู ุจุฏุงุฑ ุจุฑ ูุง ุณุฑุงุณุช',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุฏุฏุงุฑ ฺฉุฑุฏ',\n",
       " 'ุจุฑุงู ุจุงุฑู ู ุฑุง ู ุจุง ุงู ุจู ฺูฺฏ',\n",
       " 'ุจู ุงุฑุงู ุจู ุฏุฏุงุฑ ุจุฑ ุณุฑ ุจุฏุฑุฏ',\n",
       " 'ฺฉู ุจุง ุชู ูฺฏุฑุฏุงู ุจุฑุงูุฏ ุจู ุฏุณุช',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุฏุฏุงุฑ ฺฉูุฑู',\n",
       " 'ุจุฑุงู ุชุงุฌ ู ุชุฎุช ู ุจู ุฑูุฒ ูุงุฒ',\n",
       " 'ุจู ุงุฑุงู ุจู ุฏุฏุงุฑ ุจุฑ ุณุฑ ฺฏุฑู',\n",
       " 'ุจู ฺฏุฑุฏ ุงูุฏุฑูู ุดุงู ุจุฑ ูุง ูุฏุฏ',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุฑ ูุฑุฏ ุจุฑ ูพุด ฺฉุฑุฏ']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "############################# Attention ############################\n",
    "####################################################################\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "####################################################################\n",
    "############################# Train ################################\n",
    "####################################################################\n",
    "\n",
    "# inputs to the model\n",
    "\n",
    "Encoder_Input = tf.placeholder(dtype=tf.int32, shape=(data_in_batch, None))\n",
    "Decoder_Input = tf.placeholder(dtype=tf.int32, shape=(data_in_batch, None))\n",
    "Decoder_Target = tf.placeholder(dtype=tf.int32, shape=(data_in_batch, None))\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([len(chrs) + 3, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "EI = tf.nn.embedding_lookup(embeddings, Encoder_Input)\n",
    "DI = tf.nn.embedding_lookup(embeddings, Decoder_Input)\n",
    "DT = tf.one_hot(Decoder_Target, depth = int(len(chrs) + 3), on_value=1.0, off_value=0.0, axis=-1)\n",
    "\n",
    "#########################################################################\n",
    "############################### Encoder #################################\n",
    "#########################################################################\n",
    "\n",
    "# encoder lstm\n",
    "cell_encoder = tf.contrib.rnn.LSTMCell(num_units=hidden_num, \n",
    "                                       use_peepholes=True, \n",
    "                                       initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                       activation='tanh',\n",
    "                                       name='cell_encoder')\n",
    "\n",
    "all_states_ec, final_state_ec = tf.nn.dynamic_rnn(cell=cell_encoder, \n",
    "                                                  inputs=EI, \n",
    "                                                  initial_state=None,\n",
    "                                                  time_major=False,dtype=tf.float32)\n",
    "\n",
    "#########################################################################\n",
    "############################### Decoder #################################\n",
    "#########################################################################\n",
    "\n",
    "# decoder lstm\n",
    "cell_decoder = tf.contrib.rnn.LSTMCell(num_units=hidden_num, \n",
    "                                       use_peepholes=True, \n",
    "                                       initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                       activation='tanh',\n",
    "                                       name='cell_decoder')\n",
    "# add attention to decoder\n",
    "BahdanauAttention = tf.contrib.seq2seq.BahdanauAttention(num_units = total_num_char, memory = all_states_ec,\n",
    "                                                         name = 'BahdanauAttention')\n",
    "AttentionWrapper = tf.contrib.seq2seq.AttentionWrapper(cell=cell_decoder, attention_mechanism = BahdanauAttention, \n",
    "                                                       name='AttentionWrapper')\n",
    "\n",
    "initial_state = AttentionWrapper.zero_state(data_in_batch, tf.float32).clone(cell_state = final_state_ec)\n",
    "\n",
    "all_states_dec, final_state_dec = tf.nn.dynamic_rnn(cell=AttentionWrapper, \n",
    "                                                     inputs=DI, \n",
    "                                                     initial_state=initial_state, \n",
    "                                                     time_major=False,\n",
    "                                                     dtype=tf.float32)\n",
    "# Outputs\n",
    "\n",
    "W = tf.get_variable('W', shape=(hidden_num,int(len(chrs) + 3)),initializer=tf.contrib.layers.xavier_initializer(), dtype=tf.float32)\n",
    "b = tf.get_variable('b', shape=(int(len(chrs) + 3),),initializer=tf.contrib.layers.xavier_initializer(), dtype=tf.float32)\n",
    "\n",
    "Decoder_logit = tf.einsum('ijk,kl', all_states_dec, W)+b\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=DT, logits=Decoder_logit, dim=-1))\n",
    "\n",
    "tf_lr = tf.placeholder(dtype=tf.float32)\n",
    "train_opt = tf.train.RMSPropOptimizer(learning_rate = tf_lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "############################# Test #################################\n",
    "####################################################################\n",
    "\n",
    "# placeholders for feeding in test time:\n",
    "Encoder_Input_test = tf.placeholder(dtype=tf.int32, shape=(data_in_batch, None))\n",
    "Decoder_Input_test = tf.placeholder(dtype=tf.int32, shape=(data_in_batch, None))\n",
    "\n",
    "# Initial_dec_h : activations of hidden layer of LSTM cell\n",
    "# Initial_dec_c : is final output, which can potentially be transfromed with some wrapper\n",
    "Initial_dec_h = tf.placeholder(dtype=tf.float32, shape=(data_in_batch, hidden_num))\n",
    "Initial_dec_c = tf.placeholder(dtype=tf.float32, shape=(data_in_batch, hidden_num))\n",
    "\n",
    "initial_state = tf.nn.rnn_cell.LSTMStateTuple(Initial_dec_c, Initial_dec_h)\n",
    "\n",
    "EI_test = tf.nn.embedding_lookup(embeddings, Encoder_Input_test)\n",
    "DI_test = tf.nn.embedding_lookup(embeddings, Decoder_Input_test)\n",
    "\n",
    "all_states_ec_test, final_state_ec_test = tf.nn.dynamic_rnn(cell=cell_encoder, \n",
    "                                             inputs=EI_test, \n",
    "                                             initial_state=None, \n",
    "                                             time_major=False,\n",
    "                                             dtype=tf.float32)\n",
    "\n",
    "Initial_state = AttentionWrapper.zero_state(data_in_batch, tf.float32).clone(cell_state = initial_state)\n",
    "\n",
    "\n",
    "all_states_dec_test, final_state_dec_test = tf.nn.dynamic_rnn(cell=AttentionWrapper, \n",
    "                                                             inputs=DI_test, \n",
    "                                                             initial_state=Initial_state, \n",
    "                                                             time_major=False,\n",
    "                                                             dtype=tf.float32)\n",
    "\n",
    "logits_test = tf.einsum('ijk,kl', all_states_dec_test, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 ====== Loss Train: 3.6681 ====== Loss Test: 3.6682\n",
      "Epoch   0 ====== Loss Train: 2.7769 ====== Loss Test: 2.7727\n",
      "Epoch   0 ====== Loss Train: 2.1845 ====== Loss Test: 2.5462\n",
      "Epoch   1 ====== Loss Train: 2.0470 ====== Loss Test: 2.7782\n",
      "Epoch   1 ====== Loss Train: 1.8412 ====== Loss Test: 3.1054\n",
      "Epoch   1 ====== Loss Train: 1.7965 ====== Loss Test: 3.4333\n",
      "Epoch   2 ====== Loss Train: 1.7435 ====== Loss Test: 3.4745\n",
      "Epoch   2 ====== Loss Train: 1.7200 ====== Loss Test: 3.5588\n",
      "Epoch   2 ====== Loss Train: 1.6645 ====== Loss Test: 3.7425\n",
      "Epoch   3 ====== Loss Train: 1.6136 ====== Loss Test: 3.4392\n",
      "Epoch   3 ====== Loss Train: 1.6450 ====== Loss Test: 3.7282\n",
      "Epoch   3 ====== Loss Train: 1.6717 ====== Loss Test: 3.9540\n",
      "Epoch   4 ====== Loss Train: 1.5448 ====== Loss Test: 3.7889\n",
      "Epoch   4 ====== Loss Train: 1.5193 ====== Loss Test: 3.8673\n",
      "Epoch   4 ====== Loss Train: 1.4901 ====== Loss Test: 4.2002\n",
      "Epoch   5 ====== Loss Train: 1.4558 ====== Loss Test: 4.0348\n",
      "Epoch   5 ====== Loss Train: 1.4716 ====== Loss Test: 3.9839\n",
      "Epoch   5 ====== Loss Train: 1.4376 ====== Loss Test: 4.3222\n",
      "Epoch   6 ====== Loss Train: 1.4112 ====== Loss Test: 4.2070\n",
      "Epoch   6 ====== Loss Train: 1.4022 ====== Loss Test: 4.2542\n",
      "Epoch   6 ====== Loss Train: 1.3474 ====== Loss Test: 4.4412\n",
      "Epoch   7 ====== Loss Train: 1.3733 ====== Loss Test: 4.4575\n",
      "Epoch   7 ====== Loss Train: 1.3753 ====== Loss Test: 4.5186\n",
      "Epoch   7 ====== Loss Train: 1.3100 ====== Loss Test: 4.6387\n",
      "Epoch   8 ====== Loss Train: 1.3786 ====== Loss Test: 4.4795\n",
      "Epoch   8 ====== Loss Train: 1.3126 ====== Loss Test: 4.5787\n",
      "Epoch   8 ====== Loss Train: 1.3571 ====== Loss Test: 4.7315\n",
      "Epoch   9 ====== Loss Train: 1.2505 ====== Loss Test: 4.6887\n",
      "Epoch   9 ====== Loss Train: 1.3044 ====== Loss Test: 4.5880\n",
      "Epoch   9 ====== Loss Train: 1.2991 ====== Loss Test: 4.9317\n",
      "Epoch  10 ====== Loss Train: 1.2192 ====== Loss Test: 5.0286\n",
      "Epoch  10 ====== Loss Train: 1.2397 ====== Loss Test: 4.9351\n",
      "Epoch  10 ====== Loss Train: 1.2418 ====== Loss Test: 5.0387\n",
      "Epoch  11 ====== Loss Train: 1.1756 ====== Loss Test: 4.9885\n",
      "Epoch  11 ====== Loss Train: 1.2066 ====== Loss Test: 4.9864\n",
      "Epoch  11 ====== Loss Train: 1.1986 ====== Loss Test: 5.0514\n",
      "Epoch  12 ====== Loss Train: 1.1595 ====== Loss Test: 4.9550\n",
      "Epoch  12 ====== Loss Train: 1.1935 ====== Loss Test: 5.0807\n",
      "Epoch  12 ====== Loss Train: 1.1705 ====== Loss Test: 5.0501\n",
      "Epoch  13 ====== Loss Train: 1.1478 ====== Loss Test: 5.2154\n",
      "Epoch  13 ====== Loss Train: 1.2130 ====== Loss Test: 5.1170\n",
      "Epoch  13 ====== Loss Train: 1.1940 ====== Loss Test: 5.1225\n",
      "Epoch  14 ====== Loss Train: 1.1346 ====== Loss Test: 5.3070\n",
      "Epoch  14 ====== Loss Train: 1.1641 ====== Loss Test: 5.2976\n",
      "Epoch  14 ====== Loss Train: 1.1466 ====== Loss Test: 5.2770\n",
      "Epoch  15 ====== Loss Train: 1.1204 ====== Loss Test: 5.4175\n",
      "Epoch  15 ====== Loss Train: 1.1559 ====== Loss Test: 5.3639\n",
      "Epoch  15 ====== Loss Train: 1.1468 ====== Loss Test: 5.4065\n",
      "Epoch  16 ====== Loss Train: 1.3520 ====== Loss Test: 5.3377\n",
      "Epoch  16 ====== Loss Train: 1.1411 ====== Loss Test: 5.2944\n",
      "Epoch  16 ====== Loss Train: 1.1232 ====== Loss Test: 5.3760\n",
      "Epoch  17 ====== Loss Train: 1.0984 ====== Loss Test: 5.3867\n",
      "Epoch  17 ====== Loss Train: 1.1363 ====== Loss Test: 5.3258\n",
      "Epoch  17 ====== Loss Train: 1.1157 ====== Loss Test: 5.5671\n",
      "Epoch  18 ====== Loss Train: 1.0827 ====== Loss Test: 5.5572\n",
      "Epoch  18 ====== Loss Train: 1.1208 ====== Loss Test: 5.5110\n",
      "Epoch  18 ====== Loss Train: 1.1004 ====== Loss Test: 5.6537\n",
      "Epoch  19 ====== Loss Train: 1.0724 ====== Loss Test: 5.5446\n",
      "Epoch  19 ====== Loss Train: 1.1064 ====== Loss Test: 5.5195\n",
      "Epoch  19 ====== Loss Train: 1.1034 ====== Loss Test: 5.7579\n",
      "Epoch  20 ====== Loss Train: 1.0637 ====== Loss Test: 5.5848\n",
      "Epoch  20 ====== Loss Train: 1.0961 ====== Loss Test: 5.7738\n",
      "Epoch  20 ====== Loss Train: 1.0796 ====== Loss Test: 5.8731\n",
      "Epoch  21 ====== Loss Train: 1.1665 ====== Loss Test: 5.5153\n",
      "Epoch  21 ====== Loss Train: 1.0797 ====== Loss Test: 5.7255\n",
      "Epoch  21 ====== Loss Train: 1.0709 ====== Loss Test: 5.8848\n",
      "Epoch  22 ====== Loss Train: 1.0445 ====== Loss Test: 5.8365\n",
      "Epoch  22 ====== Loss Train: 1.0732 ====== Loss Test: 5.8187\n",
      "Epoch  22 ====== Loss Train: 1.0635 ====== Loss Test: 5.9261\n",
      "Epoch  23 ====== Loss Train: 1.0387 ====== Loss Test: 5.7909\n",
      "Epoch  23 ====== Loss Train: 1.0652 ====== Loss Test: 5.7808\n",
      "Epoch  23 ====== Loss Train: 1.0571 ====== Loss Test: 5.8756\n",
      "Epoch  24 ====== Loss Train: 1.0331 ====== Loss Test: 6.0482\n",
      "Epoch  24 ====== Loss Train: 1.0642 ====== Loss Test: 5.9160\n",
      "Epoch  24 ====== Loss Train: 1.0533 ====== Loss Test: 6.0227\n",
      "Epoch  25 ====== Loss Train: 1.0373 ====== Loss Test: 5.9607\n",
      "Epoch  25 ====== Loss Train: 1.0592 ====== Loss Test: 5.8919\n",
      "Epoch  25 ====== Loss Train: 1.0470 ====== Loss Test: 5.9109\n",
      "Epoch  26 ====== Loss Train: 1.0216 ====== Loss Test: 6.0815\n",
      "Epoch  26 ====== Loss Train: 1.0521 ====== Loss Test: 5.9581\n",
      "Epoch  26 ====== Loss Train: 1.0397 ====== Loss Test: 6.0668\n",
      "Epoch  27 ====== Loss Train: 1.0162 ====== Loss Test: 6.1089\n",
      "Epoch  27 ====== Loss Train: 1.0448 ====== Loss Test: 5.9857\n",
      "Epoch  27 ====== Loss Train: 1.0341 ====== Loss Test: 6.0881\n",
      "Epoch  28 ====== Loss Train: 1.0145 ====== Loss Test: 6.1650\n",
      "Epoch  28 ====== Loss Train: 1.0398 ====== Loss Test: 6.0135\n",
      "Epoch  28 ====== Loss Train: 1.0339 ====== Loss Test: 6.0167\n",
      "Epoch  29 ====== Loss Train: 1.0088 ====== Loss Test: 6.0138\n",
      "Epoch  29 ====== Loss Train: 1.0341 ====== Loss Test: 5.9976\n",
      "Epoch  29 ====== Loss Train: 1.0268 ====== Loss Test: 6.1821\n",
      "Epoch  30 ====== Loss Train: 1.0074 ====== Loss Test: 6.0860\n",
      "Epoch  30 ====== Loss Train: 1.0275 ====== Loss Test: 6.0064\n",
      "Epoch  30 ====== Loss Train: 1.0170 ====== Loss Test: 6.4436\n",
      "Epoch  31 ====== Loss Train: 0.9958 ====== Loss Test: 6.2648\n",
      "Epoch  31 ====== Loss Train: 1.0184 ====== Loss Test: 6.1190\n",
      "Epoch  31 ====== Loss Train: 1.0140 ====== Loss Test: 6.4846\n",
      "Epoch  32 ====== Loss Train: 0.9923 ====== Loss Test: 6.2536\n",
      "Epoch  32 ====== Loss Train: 1.0159 ====== Loss Test: 6.2908\n",
      "Epoch  32 ====== Loss Train: 1.0098 ====== Loss Test: 6.4681\n",
      "Epoch  33 ====== Loss Train: 0.9935 ====== Loss Test: 6.2203\n",
      "Epoch  33 ====== Loss Train: 1.0128 ====== Loss Test: 6.2775\n",
      "Epoch  33 ====== Loss Train: 1.0064 ====== Loss Test: 6.5064\n",
      "Epoch  34 ====== Loss Train: 0.9863 ====== Loss Test: 6.3802\n",
      "Epoch  34 ====== Loss Train: 1.0090 ====== Loss Test: 6.3846\n",
      "Epoch  34 ====== Loss Train: 1.0017 ====== Loss Test: 6.3860\n",
      "Epoch  35 ====== Loss Train: 0.9836 ====== Loss Test: 6.4881\n",
      "Epoch  35 ====== Loss Train: 1.0041 ====== Loss Test: 6.3814\n",
      "Epoch  35 ====== Loss Train: 0.9981 ====== Loss Test: 6.6213\n",
      "Epoch  36 ====== Loss Train: 0.9829 ====== Loss Test: 6.5307\n",
      "Epoch  36 ====== Loss Train: 1.0015 ====== Loss Test: 6.2539\n",
      "Epoch  36 ====== Loss Train: 0.9972 ====== Loss Test: 6.5254\n",
      "Epoch  37 ====== Loss Train: 0.9797 ====== Loss Test: 6.5276\n",
      "Epoch  37 ====== Loss Train: 0.9956 ====== Loss Test: 6.2338\n",
      "Epoch  37 ====== Loss Train: 0.9929 ====== Loss Test: 6.5570\n",
      "Epoch  38 ====== Loss Train: 0.9742 ====== Loss Test: 6.3546\n",
      "Epoch  38 ====== Loss Train: 0.9970 ====== Loss Test: 6.3322\n",
      "Epoch  38 ====== Loss Train: 0.9909 ====== Loss Test: 6.4862\n",
      "Epoch  39 ====== Loss Train: 0.9718 ====== Loss Test: 6.5831\n",
      "Epoch  39 ====== Loss Train: 0.9912 ====== Loss Test: 6.3837\n",
      "Epoch  39 ====== Loss Train: 0.9864 ====== Loss Test: 6.5733\n",
      "Epoch  40 ====== Loss Train: 0.9705 ====== Loss Test: 6.5067\n",
      "Epoch  40 ====== Loss Train: 0.9889 ====== Loss Test: 6.6668\n",
      "Epoch  40 ====== Loss Train: 0.9813 ====== Loss Test: 6.6324\n",
      "Epoch  41 ====== Loss Train: 0.9654 ====== Loss Test: 6.6099\n",
      "Epoch  41 ====== Loss Train: 0.9843 ====== Loss Test: 6.7812\n",
      "Epoch  41 ====== Loss Train: 0.9788 ====== Loss Test: 6.6026\n",
      "Epoch  42 ====== Loss Train: 0.9602 ====== Loss Test: 6.7938\n",
      "Epoch  42 ====== Loss Train: 0.9819 ====== Loss Test: 6.5357\n",
      "Epoch  42 ====== Loss Train: 0.9779 ====== Loss Test: 6.8648\n",
      "Epoch  43 ====== Loss Train: 0.9840 ====== Loss Test: 6.5092\n",
      "Epoch  43 ====== Loss Train: 0.9801 ====== Loss Test: 6.7007\n",
      "Epoch  43 ====== Loss Train: 0.9748 ====== Loss Test: 6.7454\n",
      "Epoch  44 ====== Loss Train: 0.9576 ====== Loss Test: 6.7181\n",
      "Epoch  44 ====== Loss Train: 0.9778 ====== Loss Test: 6.8823\n",
      "Epoch  44 ====== Loss Train: 0.9718 ====== Loss Test: 6.8711\n",
      "Epoch  45 ====== Loss Train: 0.9541 ====== Loss Test: 6.8779\n",
      "Epoch  45 ====== Loss Train: 0.9742 ====== Loss Test: 6.8254\n",
      "Epoch  45 ====== Loss Train: 0.9711 ====== Loss Test: 6.7956\n",
      "Epoch  46 ====== Loss Train: 0.9541 ====== Loss Test: 6.7387\n",
      "Epoch  46 ====== Loss Train: 0.9734 ====== Loss Test: 6.7684\n",
      "Epoch  46 ====== Loss Train: 0.9686 ====== Loss Test: 6.7840\n",
      "Epoch  47 ====== Loss Train: 0.9739 ====== Loss Test: 6.7390\n",
      "Epoch  47 ====== Loss Train: 0.9718 ====== Loss Test: 6.7250\n",
      "Epoch  47 ====== Loss Train: 0.9665 ====== Loss Test: 6.8799\n",
      "Epoch  48 ====== Loss Train: 0.9492 ====== Loss Test: 6.8919\n",
      "Epoch  48 ====== Loss Train: 0.9704 ====== Loss Test: 6.7445\n",
      "Epoch  48 ====== Loss Train: 0.9644 ====== Loss Test: 6.9815\n",
      "Epoch  49 ====== Loss Train: 0.9491 ====== Loss Test: 6.8592\n",
      "Epoch  49 ====== Loss Train: 0.9680 ====== Loss Test: 6.6841\n",
      "Epoch  49 ====== Loss Train: 0.9625 ====== Loss Test: 7.0217\n",
      "Epoch  50 ====== Loss Train: 0.9459 ====== Loss Test: 6.9502\n",
      "Epoch  50 ====== Loss Train: 0.9647 ====== Loss Test: 6.8763\n",
      "Epoch  50 ====== Loss Train: 0.9586 ====== Loss Test: 7.0521\n",
      "Epoch  51 ====== Loss Train: 0.9420 ====== Loss Test: 6.8730\n",
      "Epoch  51 ====== Loss Train: 0.9618 ====== Loss Test: 6.7512\n",
      "Epoch  51 ====== Loss Train: 0.9561 ====== Loss Test: 7.0121\n",
      "Epoch  52 ====== Loss Train: 0.9406 ====== Loss Test: 6.8287\n",
      "Epoch  52 ====== Loss Train: 0.9608 ====== Loss Test: 7.0336\n",
      "Epoch  52 ====== Loss Train: 0.9549 ====== Loss Test: 7.0283\n",
      "Epoch  53 ====== Loss Train: 0.9394 ====== Loss Test: 6.8727\n",
      "Epoch  53 ====== Loss Train: 0.9591 ====== Loss Test: 6.9792\n",
      "Epoch  53 ====== Loss Train: 0.9533 ====== Loss Test: 7.2560\n",
      "Epoch  54 ====== Loss Train: 0.9384 ====== Loss Test: 6.9765\n",
      "Epoch  54 ====== Loss Train: 0.9578 ====== Loss Test: 6.8528\n",
      "Epoch  54 ====== Loss Train: 0.9516 ====== Loss Test: 7.1146\n",
      "Epoch  55 ====== Loss Train: 0.9371 ====== Loss Test: 7.1490\n",
      "Epoch  55 ====== Loss Train: 0.9565 ====== Loss Test: 6.9204\n",
      "Epoch  55 ====== Loss Train: 0.9505 ====== Loss Test: 7.1334\n",
      "Epoch  56 ====== Loss Train: 0.9357 ====== Loss Test: 7.0209\n",
      "Epoch  56 ====== Loss Train: 0.9552 ====== Loss Test: 6.8894\n",
      "Epoch  56 ====== Loss Train: 0.9486 ====== Loss Test: 7.0117\n",
      "Epoch  57 ====== Loss Train: 0.9342 ====== Loss Test: 7.0988\n",
      "Epoch  57 ====== Loss Train: 0.9552 ====== Loss Test: 7.0591\n",
      "Epoch  57 ====== Loss Train: 0.9473 ====== Loss Test: 7.3799\n",
      "Epoch  58 ====== Loss Train: 0.9338 ====== Loss Test: 7.2757\n",
      "Epoch  58 ====== Loss Train: 0.9530 ====== Loss Test: 7.2165\n",
      "Epoch  58 ====== Loss Train: 0.9463 ====== Loss Test: 7.2045\n",
      "Epoch  59 ====== Loss Train: 0.9340 ====== Loss Test: 7.0191\n",
      "Epoch  59 ====== Loss Train: 0.9511 ====== Loss Test: 7.1304\n",
      "Epoch  59 ====== Loss Train: 0.9458 ====== Loss Test: 7.3797\n",
      "Epoch  60 ====== Loss Train: 0.9314 ====== Loss Test: 6.9863\n",
      "Epoch  60 ====== Loss Train: 0.9494 ====== Loss Test: 7.2030\n",
      "Epoch  60 ====== Loss Train: 0.9415 ====== Loss Test: 7.0906\n",
      "Epoch  61 ====== Loss Train: 0.9283 ====== Loss Test: 7.0640\n",
      "Epoch  61 ====== Loss Train: 0.9474 ====== Loss Test: 7.2311\n",
      "Epoch  61 ====== Loss Train: 0.9399 ====== Loss Test: 7.2910\n",
      "Epoch  62 ====== Loss Train: 0.9282 ====== Loss Test: 7.2864\n",
      "Epoch  62 ====== Loss Train: 0.9464 ====== Loss Test: 7.0858\n",
      "Epoch  62 ====== Loss Train: 0.9386 ====== Loss Test: 7.3670\n",
      "Epoch  63 ====== Loss Train: 0.9273 ====== Loss Test: 7.2845\n",
      "Epoch  63 ====== Loss Train: 0.9453 ====== Loss Test: 7.0206\n",
      "Epoch  63 ====== Loss Train: 0.9376 ====== Loss Test: 7.3162\n",
      "Epoch  64 ====== Loss Train: 0.9265 ====== Loss Test: 7.1297\n",
      "Epoch  64 ====== Loss Train: 0.9441 ====== Loss Test: 7.1793\n",
      "Epoch  64 ====== Loss Train: 0.9368 ====== Loss Test: 7.4525\n",
      "Epoch  65 ====== Loss Train: 0.9254 ====== Loss Test: 7.2451\n",
      "Epoch  65 ====== Loss Train: 0.9431 ====== Loss Test: 7.2928\n",
      "Epoch  65 ====== Loss Train: 0.9351 ====== Loss Test: 7.2638\n",
      "Epoch  66 ====== Loss Train: 0.9245 ====== Loss Test: 7.3967\n",
      "Epoch  66 ====== Loss Train: 0.9425 ====== Loss Test: 7.0256\n",
      "Epoch  66 ====== Loss Train: 0.9341 ====== Loss Test: 7.3670\n",
      "Epoch  67 ====== Loss Train: 0.9242 ====== Loss Test: 7.3560\n",
      "Epoch  67 ====== Loss Train: 0.9408 ====== Loss Test: 7.1841\n",
      "Epoch  67 ====== Loss Train: 0.9333 ====== Loss Test: 7.3901\n",
      "Epoch  68 ====== Loss Train: 0.9234 ====== Loss Test: 7.5149\n",
      "Epoch  68 ====== Loss Train: 0.9402 ====== Loss Test: 7.2044\n",
      "Epoch  68 ====== Loss Train: 0.9321 ====== Loss Test: 7.2298\n",
      "Epoch  69 ====== Loss Train: 0.9229 ====== Loss Test: 7.4437\n",
      "Epoch  69 ====== Loss Train: 0.9389 ====== Loss Test: 7.2233\n",
      "Epoch  69 ====== Loss Train: 0.9308 ====== Loss Test: 7.4986\n",
      "Epoch  70 ====== Loss Train: 0.9220 ====== Loss Test: 7.3696\n",
      "Epoch  70 ====== Loss Train: 0.9379 ====== Loss Test: 7.2876\n",
      "Epoch  70 ====== Loss Train: 0.9288 ====== Loss Test: 7.5587\n",
      "Epoch  71 ====== Loss Train: 0.9204 ====== Loss Test: 7.5283\n",
      "Epoch  71 ====== Loss Train: 0.9355 ====== Loss Test: 7.2089\n",
      "Epoch  71 ====== Loss Train: 0.9278 ====== Loss Test: 7.2778\n",
      "Epoch  72 ====== Loss Train: 0.9198 ====== Loss Test: 7.3625\n",
      "Epoch  72 ====== Loss Train: 0.9349 ====== Loss Test: 7.4821\n",
      "Epoch  72 ====== Loss Train: 0.9269 ====== Loss Test: 7.4765\n",
      "Epoch  73 ====== Loss Train: 0.9189 ====== Loss Test: 7.2792\n",
      "Epoch  73 ====== Loss Train: 0.9337 ====== Loss Test: 7.2208\n",
      "Epoch  73 ====== Loss Train: 0.9260 ====== Loss Test: 7.5614\n",
      "Epoch  74 ====== Loss Train: 0.9181 ====== Loss Test: 7.2501\n",
      "Epoch  74 ====== Loss Train: 0.9336 ====== Loss Test: 7.2118\n",
      "Epoch  74 ====== Loss Train: 0.9249 ====== Loss Test: 7.4273\n",
      "Epoch  75 ====== Loss Train: 0.9177 ====== Loss Test: 7.3495\n",
      "Epoch  75 ====== Loss Train: 0.9325 ====== Loss Test: 7.2458\n",
      "Epoch  75 ====== Loss Train: 0.9249 ====== Loss Test: 7.3895\n",
      "Epoch  76 ====== Loss Train: 0.9171 ====== Loss Test: 7.3697\n",
      "Epoch  76 ====== Loss Train: 0.9317 ====== Loss Test: 7.1996\n",
      "Epoch  76 ====== Loss Train: 0.9242 ====== Loss Test: 7.4518\n",
      "Epoch  77 ====== Loss Train: 0.9163 ====== Loss Test: 7.4256\n",
      "Epoch  77 ====== Loss Train: 0.9311 ====== Loss Test: 7.2044\n",
      "Epoch  77 ====== Loss Train: 0.9233 ====== Loss Test: 7.5571\n",
      "Epoch  78 ====== Loss Train: 0.9158 ====== Loss Test: 7.4223\n",
      "Epoch  78 ====== Loss Train: 0.9301 ====== Loss Test: 7.5751\n",
      "Epoch  78 ====== Loss Train: 0.9227 ====== Loss Test: 7.6120\n",
      "Epoch  79 ====== Loss Train: 0.9154 ====== Loss Test: 7.4399\n",
      "Epoch  79 ====== Loss Train: 0.9295 ====== Loss Test: 7.1800\n",
      "Epoch  79 ====== Loss Train: 0.9222 ====== Loss Test: 7.6333\n",
      "Epoch  80 ====== Loss Train: 0.9144 ====== Loss Test: 7.3561\n",
      "Epoch  80 ====== Loss Train: 0.9282 ====== Loss Test: 7.2768\n",
      "Epoch  80 ====== Loss Train: 0.9201 ====== Loss Test: 7.5827\n",
      "Epoch  81 ====== Loss Train: 0.9131 ====== Loss Test: 7.3561\n",
      "Epoch  81 ====== Loss Train: 0.9271 ====== Loss Test: 7.3402\n",
      "Epoch  81 ====== Loss Train: 0.9194 ====== Loss Test: 7.5459\n",
      "Epoch  82 ====== Loss Train: 0.9131 ====== Loss Test: 7.6002\n",
      "Epoch  82 ====== Loss Train: 0.9263 ====== Loss Test: 7.3952\n",
      "Epoch  82 ====== Loss Train: 0.9188 ====== Loss Test: 7.6852\n",
      "Epoch  83 ====== Loss Train: 0.9121 ====== Loss Test: 7.3914\n",
      "Epoch  83 ====== Loss Train: 0.9258 ====== Loss Test: 7.3454\n",
      "Epoch  83 ====== Loss Train: 0.9179 ====== Loss Test: 7.6423\n",
      "Epoch  84 ====== Loss Train: 0.9117 ====== Loss Test: 7.5746\n",
      "Epoch  84 ====== Loss Train: 0.9251 ====== Loss Test: 7.5490\n",
      "Epoch  84 ====== Loss Train: 0.9173 ====== Loss Test: 7.6756\n",
      "Epoch  85 ====== Loss Train: 0.9112 ====== Loss Test: 7.5485\n",
      "Epoch  85 ====== Loss Train: 0.9251 ====== Loss Test: 7.3144\n",
      "Epoch  85 ====== Loss Train: 0.9169 ====== Loss Test: 7.6837\n",
      "Epoch  86 ====== Loss Train: 0.9106 ====== Loss Test: 7.6818\n",
      "Epoch  86 ====== Loss Train: 0.9239 ====== Loss Test: 7.4162\n",
      "Epoch  86 ====== Loss Train: 0.9161 ====== Loss Test: 7.5255\n",
      "Epoch  87 ====== Loss Train: 0.9103 ====== Loss Test: 7.3957\n",
      "Epoch  87 ====== Loss Train: 0.9236 ====== Loss Test: 7.2229\n",
      "Epoch  87 ====== Loss Train: 0.9156 ====== Loss Test: 7.7160\n",
      "Epoch  88 ====== Loss Train: 0.9098 ====== Loss Test: 7.7247\n",
      "Epoch  88 ====== Loss Train: 0.9230 ====== Loss Test: 7.6632\n",
      "Epoch  88 ====== Loss Train: 0.9152 ====== Loss Test: 7.7070\n",
      "Epoch  89 ====== Loss Train: 0.9092 ====== Loss Test: 7.3295\n",
      "Epoch  89 ====== Loss Train: 0.9224 ====== Loss Test: 7.6696\n",
      "Epoch  89 ====== Loss Train: 0.9143 ====== Loss Test: 7.6924\n",
      "Epoch  90 ====== Loss Train: 0.9088 ====== Loss Test: 7.5555\n",
      "Epoch  90 ====== Loss Train: 0.9216 ====== Loss Test: 7.4517\n",
      "Epoch  90 ====== Loss Train: 0.9131 ====== Loss Test: 7.6603\n",
      "Epoch  91 ====== Loss Train: 0.9074 ====== Loss Test: 7.7679\n",
      "Epoch  91 ====== Loss Train: 0.9207 ====== Loss Test: 7.3862\n",
      "Epoch  91 ====== Loss Train: 0.9124 ====== Loss Test: 7.5669\n",
      "Epoch  92 ====== Loss Train: 0.9071 ====== Loss Test: 7.7212\n",
      "Epoch  92 ====== Loss Train: 0.9203 ====== Loss Test: 7.4328\n",
      "Epoch  92 ====== Loss Train: 0.9121 ====== Loss Test: 7.7622\n",
      "Epoch  93 ====== Loss Train: 0.9064 ====== Loss Test: 7.6030\n",
      "Epoch  93 ====== Loss Train: 0.9198 ====== Loss Test: 7.3859\n",
      "Epoch  93 ====== Loss Train: 0.9115 ====== Loss Test: 7.6615\n",
      "Epoch  94 ====== Loss Train: 0.9064 ====== Loss Test: 7.5986\n",
      "Epoch  94 ====== Loss Train: 0.9193 ====== Loss Test: 7.5114\n",
      "Epoch  94 ====== Loss Train: 0.9113 ====== Loss Test: 7.7645\n",
      "Epoch  95 ====== Loss Train: 0.9059 ====== Loss Test: 7.6904\n",
      "Epoch  95 ====== Loss Train: 0.9189 ====== Loss Test: 7.5255\n",
      "Epoch  95 ====== Loss Train: 0.9108 ====== Loss Test: 7.6460\n",
      "Epoch  96 ====== Loss Train: 0.9054 ====== Loss Test: 7.6474\n",
      "Epoch  96 ====== Loss Train: 0.9186 ====== Loss Test: 7.6307\n",
      "Epoch  96 ====== Loss Train: 0.9102 ====== Loss Test: 7.6145\n",
      "Epoch  97 ====== Loss Train: 0.9054 ====== Loss Test: 7.7017\n",
      "Epoch  97 ====== Loss Train: 0.9181 ====== Loss Test: 7.5248\n",
      "Epoch  97 ====== Loss Train: 0.9101 ====== Loss Test: 7.9172\n",
      "Epoch  98 ====== Loss Train: 0.9047 ====== Loss Test: 7.3869\n",
      "Epoch  98 ====== Loss Train: 0.9181 ====== Loss Test: 7.4552\n",
      "Epoch  98 ====== Loss Train: 0.9093 ====== Loss Test: 7.7189\n",
      "Epoch  99 ====== Loss Train: 0.9045 ====== Loss Test: 7.5440\n",
      "Epoch  99 ====== Loss Train: 0.9175 ====== Loss Test: 7.6212\n",
      "Epoch  99 ====== Loss Train: 0.9088 ====== Loss Test: 7.7468\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "\n",
    "lr = lr_base\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for j in range(n_batch):\n",
    "        \n",
    "        # adjusting learning rete \n",
    "        if i%10==0:\n",
    "            lr = lr*lr_decay\n",
    "            if lr < lr_floor:\n",
    "                lr = lr_floor\n",
    "        mes1_minibatch = Trian_mes01[(j)*data_in_batch:min((j+1)*data_in_batch,Trian_mes01.shape[0]),:]\n",
    "        mes2_minibatch = Trian_mes02[(j)*data_in_batch:min((j+1)*data_in_batch,Trian_mes01.shape[0]),:]\n",
    "                \n",
    "        target = mes2_minibatch\n",
    "        target = np.roll(target,-1,axis=1)\n",
    "        target[0,-1] = 0\n",
    "        \n",
    "        loss_train,_ = session.run([loss,train_opt], feed_dict={Encoder_Input: mes1_minibatch,\n",
    "                                                               Decoder_Input: mes2_minibatch,\n",
    "                                                               Decoder_Target: target,\n",
    "                                                               tf_lr: lr})\n",
    "                \n",
    "        test_idx = np.random.randint(0, Test_mes01.shape[0], size = data_in_batch)\n",
    "        \n",
    "        target_test = Test_mes02[test_idx,:]\n",
    "        target_test = np.roll(target,-1,axis=1)\n",
    "        target_test[0,-1] = 0\n",
    "        \n",
    "        loss_test = session.run(loss,feed_dict={Encoder_Input: Test_mes01[test_idx,:],\n",
    "                                                    Decoder_Input: Test_mes02[test_idx,:],\n",
    "                                                    Decoder_Target: target_test})\n",
    "        \n",
    "        if j%100==0:\n",
    "            print(\"Epoch %3i\"%i, \"====== Loss Train: %.4f\" %loss_train,\"====== Loss Test: %.4f\"%loss_test)\n",
    "            train_loss = train_loss + [loss_train]\n",
    "            test_loss = test_loss + [loss_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_mes01 = Trian_mes01[0:data_in_batch,:]\n",
    "\n",
    "num_of_input = np.shape(Input_mes01)[0]\n",
    "DecInitState = session.run(final_state_ec_test, feed_dict={Encoder_Input_test: Input_mes01})\n",
    "\n",
    "# output mesra'\n",
    "Output_At = np.array([[Dict['_BOM_']]*num_of_input])\n",
    "\n",
    "logits, new_state = session.run([logits_test, final_state_dec_test], \n",
    "                                    feed_dict={Decoder_Input_test: Output_At[-1][np.newaxis].T,\n",
    "                                               Encoder_Input: Input_mes01,\n",
    "                                               Decoder_Input: Output_At[-1][np.newaxis].T,\n",
    "                                               Initial_dec_c: DecInitState[0],\n",
    "                                               Initial_dec_h: DecInitState[1]})\n",
    "\n",
    "# in each state we get a char\n",
    "\n",
    "Output_Char = np.argmax(logits, axis=-1)\n",
    "\n",
    "Output_At = np.concatenate((Output_At,Output_Char.T))\n",
    "counter = 0 \n",
    "\n",
    "stop_length = 0\n",
    "\n",
    "while not all(Output[-1] == Dict['_PAD_'])and stop_length < 50:\n",
    "    stop_length += 1\n",
    "    logits, new_state = session.run([logits_test, final_state_dec_test], \n",
    "                                        feed_dict={Decoder_Input_test: Output_At[-1][np.newaxis].T,\n",
    "                                                   Encoder_Input: Input_mes01,\n",
    "                                                   Decoder_Input: Output_At[-1][np.newaxis].T,\n",
    "                                                   Initial_dec_h: new_state[0][1],\n",
    "                                                   Initial_dec_c: new_state[0][0]})\n",
    "    Output_Char = np.argmax(logits, axis=-1)\n",
    "    Output_At = np.concatenate((Output_At,Output_Char.T))\n",
    "\n",
    "Output_At = Output_At.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1 = plt.figure()  \n",
    "plt.plot(range(len(train_loss)), train_loss, label='loss on train')\n",
    "plt.plot(range(len(test_loss)), test_loss, label='loss on test')\n",
    "\n",
    "plt.xlabel('iteration x 100')\n",
    "plt.ylabel('loss')\n",
    "plt.title(\"\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_At_of_model = [''.join([InvDict[x] for x in Output[idx,:] if not x==0 and not x==1 and not x==2]) for idx in range(50)]\n",
    "np.save('Output_At_of_model', Output_At_of_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ุจู ุงุฑุงู ุจู ุฏุฏุงุฑ ุจุง ุงู ุฒูู',\n",
       " 'ุจุฑุงู ุณุงู ฺฉู ุจุฑ ุชุฎุช ุดุงู ุจุฌูุด',\n",
       " 'ุจู ุงุฑุงู ุจุฑุงูุฏ ุฒ ุงุฑุงู ุจุฑู',\n",
       " 'ุจุฑุงู ุชุฎุช ุดุงู ุจุฑุงูุฏ ุฒ ุฏุงุฑ',\n",
       " 'ุจุฑุงู ุชุงุฌ ู ุชุฎุช ู ุจู ุฑู ู ุณูพุงู',\n",
       " 'ุจู ูพุด ุงูุฏุฑูู ุดุงู ุจุง ุงู ุจุฏ',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุฏุฏุงุฑ ฺฉุฑุฏ',\n",
       " 'ุจู ูพุด ุณูพู ุฑุง ุจู ุฏู ุจุฑ ฺฏุฐุดุช',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุฏูุงุฑ ุฎุดฺฉ',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุฏุฏุงุฑ ุดุงุฏ',\n",
       " 'ุจู ูพุด ุงูุฏุฑูู ุฑุง ุจุงุฏ ุดุฏุงุฑ',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุงุฑุงู ููุงูุฏ',\n",
       " 'ุจู ูพุด ุณูพู ุฑุง ุจู ุฏุฏุงุฑ ฺฉุฑุฏ',\n",
       " 'ุจู ูพุด ุงูุฏุฑูู ุฑุง ุงู ุงูุชุงู',\n",
       " 'ุจุฑุงู ูุงููุฑ ุดูุฑุงุฑ ุฌูุงู',\n",
       " 'ุจุฑุงู ูุงููุฑ ุดูุฑุงุฑ ุณูพ',\n",
       " 'ุจู ุฏุฏุงุฑ ุงู ุจุฑ ุณุฑ ุงุฒ ุฏุฑุฏ ู ุฌูุช',\n",
       " 'ฺฉู ุงุฒ ุฑุงู ุจุฏุงุฑ ุจุฑ ูุง ุณุฑุงู',\n",
       " 'ุจู ูพุด ุณูพู ุฑุง ุจู ุฏู ุจุฑ ูู ุงู',\n",
       " 'ุจู ุงุฑุงู ุจู ุงุฑุงู ูุจุงุฏ ูู',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุฑ ุณุฑ ุจุฑุงูุฏ ุฒ ุฎุดุช',\n",
       " 'ุจู ูพุด ุงูุฏุฑูู ุฑุง ุจุงุฏ ุดุฏุงุฑ',\n",
       " 'ูุจูุฏ ุจู ุฏูุงุฑ ู ุจุง ุงู ุจู ุฏุณุช',\n",
       " 'ุจู ูพุด ุณูพู ุฑุง ุจู ุฏู ุจุฑ ฺฏุฑุฒ',\n",
       " 'ุจู ุฏุฏุงุฑ ุงู ุจุฑ ุณุฑ ุงุฒ ุฏุงุฏ ุฑุงู',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุงุฑุงู ูฺฏุฑ',\n",
       " 'ุณุฑ ุชุฎุช ุดุงู ุจู ุงุฑุงู ุจู ุฏุดุช',\n",
       " 'ุจุฑุงู ูุงููุฑ ุดูุฑุงุฑ ุฌูุง',\n",
       " 'ุจู ุงุฑุงู ุจู ุฏุฏุงุฑ ุจุฑ ุณุฑ ุจูุฏ',\n",
       " 'ฺฉู ุง ูุงูุฏุงุฑุงู ู ุจุฑ ูพุด ฺฏุฑู',\n",
       " 'ุจุฑุงู ุจุงุฑู ู ุฑุง ู ุจุง ุงู ุฑุง',\n",
       " 'ุจู ูพุด ุณูพู ุฑุง ุจู ุงุฑุงู ุฒูู',\n",
       " 'ุจู ฺฏุฑุฏ ุงูุฏุฑูู ุดุงู ุจุง ุงู ุจุฏุฏ',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุฏุฏุงุฑ ูุณุช',\n",
       " 'ุจุฑุงู ูุงููุฑ ุดูุฑุงุฑ ุฌูุง',\n",
       " 'ฺฉู ุง ูุงูุฏุงุฑุงู ู ุงู ูุงูุฏ',\n",
       " 'ุจู ุงุฑุงู ุจู ุฏุฏุงุฑ ุจุง ุงู ุจุฑู',\n",
       " 'ุจุฑุงู ุจุงุฑู ุจุฑ ูุฑุฏ ุจุฑ ูพุด ุจุงุฏ',\n",
       " 'ุจู ุฏูุงุฑ ู ุจุง ุงู ุจู ุฏุฏุงุฑ ฺฉุฑุฏ',\n",
       " 'ุจุฑุงู ูุงููุฑ ุดูุฑุงุฑ ุฌูุงุฏ',\n",
       " 'ฺฉู ุงุฒ ุฑุงู ุจุฏุงุฑ ุจุฑ ูุง ุณุฑุงุณุช',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุฏุฏุงุฑ ฺฉุฑุฏ',\n",
       " 'ุจุฑุงู ุจุงุฑู ู ุฑุง ู ุจุง ุงู ุจู ฺูฺฏ',\n",
       " 'ุจู ุงุฑุงู ุจู ุฏุฏุงุฑ ุจุฑ ุณุฑ ุจุฏุฑุฏ',\n",
       " 'ฺฉู ุจุง ุชู ูฺฏุฑุฏุงู ุจุฑุงูุฏ ุจู ุฏุณุช',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุง ุงู ุจู ุฏุฏุงุฑ ฺฉูุฑู',\n",
       " 'ุจุฑุงู ุชุงุฌ ู ุชุฎุช ู ุจู ุฑูุฒ ูุงุฒ',\n",
       " 'ุจู ุงุฑุงู ุจู ุฏุฏุงุฑ ุจุฑ ุณุฑ ฺฏุฑู',\n",
       " 'ุจู ฺฏุฑุฏ ุงูุฏุฑูู ุดุงู ุจุฑ ูุง ูุฏุฏ',\n",
       " 'ุจู ุฏุฏุงุฑ ุจุฑ ูุฑุฏ ุจุฑ ูพุด ฺฉุฑุฏ']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Output_At_of_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
